{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Models\n",
    "\n",
    "## This Juypter Notebook was created to easily load the models and set/change the directories to test the models, all in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows evaluate.ipynb to access modules in \"sibling\" directories\n",
    "import sys\n",
    "sys.path.append(\"./\") # Run this from parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lchris/.pyenv/versions/3.9.1/lib/python3.9/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.1` and `torch==1.8.1` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from database import Database\n",
    "from models.altered_xception import AlteredXception\n",
    "from models.faster_r_cnn import FasterRCNN\n",
    "from models.rmac_model import RMACModel\n",
    "from utils import grab_images_and_paths, grab_all_image_paths, resize_images, DATABASE_PATH\n",
    "from models.query import query_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Models and Data\n",
    "> Note: if you get any error surrounding a missing layer, a simple restart of the kernel should fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 13:31:34.639008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/models/rmac_model.py:120: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  b = (W - wl) / (l + Wd - 1)\n",
      "/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/models/rmac_model.py:125: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  b = (H-wl)/(l+Hd-1)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: if the RMAC model needs to get descriptions of many different square regions, it will take a long time to load/initialize (roughly a minute).\n",
    "ax_model = AlteredXception()\n",
    "rmac_model = RMACModel(ax_model.model.get_layer(\"conv2d_3\").output_shape[1:], 3)\n",
    "rcnn_model = FasterRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose what models to evaluate, by including or discluding them from this array\n",
    "models = [ax_model, rmac_model, rcnn_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw3_frame.jpg',\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2_clickbait.jpeg',\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2query.jpeg',\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2_distorted.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dir is the directory where our \"database\" of test images/frames and query images should exist. \n",
    "test_dir = \"/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games\"\n",
    "frame_paths, frames = grab_images_and_paths(os.path.join(test_dir, \"frames\"), num_images=1)\n",
    "query_image_paths = grab_all_image_paths(os.path.join(test_dir, \"query_images\"))\n",
    "query_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-tTaYmNB-IM',\n",
       " 'q_hZHqNnwAU',\n",
       " 'r3FaHBe1rPw',\n",
       " 'S7Nqx_pyEl0',\n",
       " 'cmh9FnLbE5s',\n",
       " 'huGVGe3Afng',\n",
       " 'kQnQrYmhXW8',\n",
       " '_lAtb14VGm8',\n",
       " '0J57bBrDDLc',\n",
       " '5X6SctBSd9A']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the YouTube video \"ground truth\" from the IDs we picked, stored in a .txt file\n",
    "ground_truth_ids = []\n",
    "ground_truth_file_path = os.path.join(test_dir, \"ground_truth.txt\")\n",
    "with open(ground_truth_file_path, \"r\") as ground_truth:\n",
    "    for line in ground_truth.readlines():\n",
    "        youtube_ground_truth_id = line.split(\"/\")[-1].split(\".\")[0]\n",
    "        ground_truth_ids.append(youtube_ground_truth_id)\n",
    "ground_truth_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models to query from the evaluation database\n",
    "database = Database(os.path.join(test_dir, \"predictions\")) # Small DB\n",
    "# database = Database(DATABASE_PATH) # Large DB \n",
    "for model in models:\n",
    "    model.database = database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model:  <models.altered_xception.AlteredXception object at 0x107c268e0>\n",
      "Current model:  <models.rmac_model.RMACModel object at 0x16d64a610>\n",
      "Current model:  <models.faster_r_cnn.FasterRCNN object at 0x16d6e35e0>\n"
     ]
    }
   ],
   "source": [
    "# Gathering predictions for each of the frames in the dataset, for each model\n",
    "if not os.path.exists(os.path.join(test_dir, \"predictions\")):\n",
    "    # Make the folder to store our representations\n",
    "    os.mkdir(os.path.join(test_dir, \"predictions\"))\n",
    "for model in models:\n",
    "    print(\"Current model: \", model)\n",
    "    if (isinstance(model, AlteredXception)) and len(database.prediction_image_paths) < len(frame_paths):\n",
    "        model_predictions = model.predict_images(frames)\n",
    "        database.store_predictions(model_predictions, frame_paths)\n",
    "    if (isinstance(model, RMACModel)) and len(database.rmac_prediction_image_paths) < len(frame_paths):\n",
    "        model_predictions = model.predict_images(frames)\n",
    "        database.store_rmac_predictions(model_predictions, frame_paths)\n",
    "    if (isinstance(model, FasterRCNN)) and len(database.object_predictions) < len(frame_paths):\n",
    "        every_10_frame_paths = frame_paths[0::10]\n",
    "        model_predictions = model.predict_image_paths(every_10_frame_paths)\n",
    "        database.store_object_data(model_predictions, every_10_frame_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_predictions(predictions):\n",
    "    model_preds = {}\n",
    "    for model in predictions:\n",
    "        video_id_preds = []\n",
    "        for video_link in predictions[model].keys():\n",
    "            video_id = video_link.split(\"/\")[-1] # We only want the last part of the path\n",
    "            video_id_preds.append(video_id)\n",
    "        model_preds[model] = video_id_preds\n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 23s 23s/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw3_frame.jpg': {'CNN': ['q_hZHqNnwAU',\n",
       "   '4Y7IB_gTCs4',\n",
       "   'kQnQrYmhXW8',\n",
       "   'f1gNEJh6eyg',\n",
       "   '0VgTeTpv48E',\n",
       "   '50DkxNJXAOY',\n",
       "   '0J57bBrDDLc',\n",
       "   'Q38a-Vt_p2E',\n",
       "   'bsNUaZf9oyg',\n",
       "   'rebo0qHivSE'],\n",
       "  'CNN + RMAC': ['q_hZHqNnwAU',\n",
       "   'lcxxGZqKj8s',\n",
       "   'U_YEdWlv5N4',\n",
       "   'bsNUaZf9oyg',\n",
       "   'vqyOxkyswVQ',\n",
       "   'I8uehX-eXoc',\n",
       "   'juyUhncX9jk',\n",
       "   'RMRuHrr7NrQ',\n",
       "   'bBIpqffkIm0',\n",
       "   '3rGvfEQ4QtQ'],\n",
       "  'RCNN': ['S7Nqx_pyEl0',\n",
       "   'NehypH0u4XY',\n",
       "   'rebo0qHivSE',\n",
       "   '5D4T7GPhQOM',\n",
       "   'f1gNEJh6eyg',\n",
       "   'WinRtwzZaLo',\n",
       "   'RvWx16ItqM8',\n",
       "   'Q38a-Vt_p2E',\n",
       "   'J35d7bA4hCE',\n",
       "   'RMRuHrr7NrQ']},\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2_clickbait.jpeg': {'CNN': ['huGVGe3Afng',\n",
       "   'WinRtwzZaLo',\n",
       "   '-tTaYmNB-IM',\n",
       "   'S7Nqx_pyEl0',\n",
       "   'HNPdE3g4jJ4',\n",
       "   'RYpNYKWhyQ8',\n",
       "   'AmHJwCJTVZc',\n",
       "   'm00Bc2ePbYY',\n",
       "   'NehypH0u4XY',\n",
       "   '4Y7IB_gTCs4'],\n",
       "  'CNN + RMAC': ['f1gNEJh6eyg',\n",
       "   'xOb1UHz5oBc',\n",
       "   'lcxxGZqKj8s',\n",
       "   'huGVGe3Afng',\n",
       "   'RYpNYKWhyQ8',\n",
       "   'fPgzH4UE80U',\n",
       "   '0VgTeTpv48E',\n",
       "   '-tTaYmNB-IM',\n",
       "   'Ma4f9EoJvEw',\n",
       "   'HNPdE3g4jJ4'],\n",
       "  'RCNN': ['XPFjFp3SF-w',\n",
       "   'avb_vbaYceY',\n",
       "   'u1kLuYhaB8Y',\n",
       "   'RMRuHrr7NrQ',\n",
       "   'Q38a-Vt_p2E',\n",
       "   'lcxxGZqKj8s',\n",
       "   'kQnQrYmhXW8',\n",
       "   'm4c0cYuRtXY',\n",
       "   'u5Elf9-rI88',\n",
       "   'cmh9FnLbE5s']},\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2query.jpeg': {'CNN': ['huGVGe3Afng',\n",
       "   'S7Nqx_pyEl0',\n",
       "   'Ma4f9EoJvEw',\n",
       "   'Tvm5Fl6pyfs',\n",
       "   '-tTaYmNB-IM',\n",
       "   'AmHJwCJTVZc',\n",
       "   'fPgzH4UE80U',\n",
       "   'bBIpqffkIm0',\n",
       "   'HNPdE3g4jJ4',\n",
       "   '0J57bBrDDLc'],\n",
       "  'CNN + RMAC': ['huGVGe3Afng',\n",
       "   'HI2s6mvkW0I',\n",
       "   'HNPdE3g4jJ4',\n",
       "   'i3P3kfQM680',\n",
       "   'Ma4f9EoJvEw',\n",
       "   'fPgzH4UE80U',\n",
       "   '3sYCSZY6K4U',\n",
       "   '0J57bBrDDLc',\n",
       "   'N5A0L7a3KXQ',\n",
       "   'Tvm5Fl6pyfs'],\n",
       "  'RCNN': ['mClkCrBAnd4',\n",
       "   'RTIKhgGXc6Y',\n",
       "   'LFZ0LwzhKZw',\n",
       "   'RMRuHrr7NrQ',\n",
       "   'bc4ukh6WRSg',\n",
       "   'NehypH0u4XY',\n",
       "   'RvWx16ItqM8',\n",
       "   '0J57bBrDDLc',\n",
       "   'r3FaHBe1rPw',\n",
       "   'cmh9FnLbE5s']},\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2_distorted.jpg': {'CNN': ['Ma4f9EoJvEw',\n",
       "   'S7Nqx_pyEl0',\n",
       "   'Tvm5Fl6pyfs',\n",
       "   'huGVGe3Afng',\n",
       "   'pHcYmKjsQCs',\n",
       "   'RYpNYKWhyQ8',\n",
       "   '0J57bBrDDLc',\n",
       "   'fPgzH4UE80U',\n",
       "   'WinRtwzZaLo',\n",
       "   'HNPdE3g4jJ4'],\n",
       "  'CNN + RMAC': ['huGVGe3Afng',\n",
       "   'HI2s6mvkW0I',\n",
       "   'HNPdE3g4jJ4',\n",
       "   'NehypH0u4XY',\n",
       "   'Ma4f9EoJvEw',\n",
       "   'i3P3kfQM680',\n",
       "   'fPgzH4UE80U',\n",
       "   'N5A0L7a3KXQ',\n",
       "   'Tvm5Fl6pyfs',\n",
       "   '2tMcneiqv-w'],\n",
       "  'RCNN': ['m4c0cYuRtXY',\n",
       "   'RTIKhgGXc6Y',\n",
       "   'bsNUaZf9oyg',\n",
       "   'fMN4TmPH2qo',\n",
       "   'vqyOxkyswVQ',\n",
       "   '5D4T7GPhQOM',\n",
       "   'lcxxGZqKj8s',\n",
       "   'XFv8X0Af_3Y',\n",
       "   'JSJnUmQvvVY',\n",
       "   'fPgzH4UE80U']}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "for query_image_path in query_image_paths:\n",
    "    results[query_image_path] = {}\n",
    "    results[query_image_path] = post_process_predictions(query_image(query_image_path, models, len(ground_truth_ids)))\n",
    "# results = query_image(query_image_paths[0], models, len(ground_truth_ids))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw3_frame.jpg': {'CNN': [2,\n",
       "   7,\n",
       "   9],\n",
       "  'CNN + RMAC': [2],\n",
       "  'RCNN': [4]},\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2_clickbait.jpeg': {'CNN': [1,\n",
       "   4,\n",
       "   6],\n",
       "  'CNN + RMAC': [1, 6],\n",
       "  'RCNN': [5, 7]},\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2query.jpeg': {'CNN': [1,\n",
       "   4,\n",
       "   6,\n",
       "   9],\n",
       "  'CNN + RMAC': [6, 9],\n",
       "  'RCNN': [3, 5, 9]},\n",
       " '/Users/lchris/Desktop/Coding/schoolprojects/comp490/COMPS/eval/test_images/test_games/query_images/mw2_distorted.jpg': {'CNN': [4,\n",
       "   6,\n",
       "   9],\n",
       "  'CNN + RMAC': [6],\n",
       "  'RCNN': []}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = {}\n",
    "for query_image_path in query_image_paths:\n",
    "    curr_results = results[query_image_path]\n",
    "    matches[query_image_path] = {}\n",
    "    for model in results[query_image_path]:\n",
    "        num_matches = []\n",
    "        for index, id in enumerate(ground_truth_ids):\n",
    "            # print(model, id, results[query_image_path][model])\n",
    "            if id in results[query_image_path][model]:\n",
    "                num_matches.append(index+1)\n",
    "        matches[query_image_path][model] = num_matches\n",
    "matches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d832aa6a45f3eb7e7bad55fdbd58c33e34218938f08b2d0a8c371f719f367be4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

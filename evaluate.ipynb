{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Models\n",
    "\n",
    "## This Juypter Notebook was created to easily load the models and set/change the directories to test the models, all in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from database import Database\n",
    "from models.altered_xception import AlteredXception\n",
    "from models.faster_r_cnn import FasterRCNN\n",
    "from models.rmac_model import RMACModel\n",
    "from utils import grab_images_and_paths, grab_all_image_paths\n",
    "from models.query import query_image\n",
    "from utils import EVAL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Models and Data\n",
    "> Note: if you get any error surrounding a missing layer, a simple restart of the kernel should fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: if the RMAC model needs to get descriptions of many different square regions, it will take a long time to load/initialize (roughly a minute).\n",
    "ax_model = AlteredXception()\n",
    "rmac_model = RMACModel(ax_model.model.get_layer(\"conv2d_3\").output_shape[1:], 3)\n",
    "rcnn_model = FasterRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose what models to evaluate, by including or discluding them from this array\n",
    "models = [ax_model, rmac_model, rcnn_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir is the directory where our \"database\" of test images/frames and query images should exist. \n",
    "test_dir = os.path.join(EVAL_PATH, \"test_dogs\")\n",
    "frame_paths, frames = grab_images_and_paths(os.path.join(test_dir, \"frames\"), num_images=1) # Number of images doesn't matter unless we want to confirm the database is complete.\n",
    "query_image_paths = grab_all_image_paths(os.path.join(test_dir, \"query_images\"))\n",
    "query_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the YouTube video \"ground truth\" from the IDs we picked, stored in a .txt file\n",
    "ground_truth_ids = []\n",
    "ground_truth_file_path = os.path.join(test_dir, \"ground_truth.txt\")\n",
    "with open(ground_truth_file_path, \"r\") as ground_truth:\n",
    "    for line in ground_truth.readlines():\n",
    "        youtube_ground_truth_id = line.split(\"/\")[-1].split(\".\")[0]\n",
    "        ground_truth_ids.append(youtube_ground_truth_id)\n",
    "ground_truth_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models to query from the evaluation database\n",
    "database = Database(os.path.join(test_dir, \"predictions\")) # Small DB\n",
    "# database = Database(DATABASE_PATH) # Large DB \n",
    "for model in models:\n",
    "    model.database = database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering predictions for each of the frames in the dataset, for each model\n",
    "if not os.path.exists(os.path.join(test_dir, \"predictions\")):\n",
    "    # Make the folder to store our representations\n",
    "    os.mkdir(os.path.join(test_dir, \"predictions\"))\n",
    "for model in models:\n",
    "    # Check that we've downloaded all the frames for each model. If not, gather and store them.\n",
    "    if (isinstance(model, AlteredXception)) and len(database.prediction_image_paths) < len(frame_paths):\n",
    "        model_predictions = model.predict_images(frames)\n",
    "        database.store_predictions(model_predictions, frame_paths)\n",
    "    if (isinstance(model, RMACModel)) and len(database.rmac_prediction_image_paths) < len(frame_paths):\n",
    "        model_predictions = model.predict_images(frames)\n",
    "        database.store_rmac_predictions(model_predictions, frame_paths)\n",
    "    if (isinstance(model, FasterRCNN)) and len(database.object_predictions) < len(frame_paths):\n",
    "        every_10_frame_paths = frame_paths[0::10]\n",
    "        model_predictions = model.predict_image_paths(every_10_frame_paths)\n",
    "        database.store_object_data(model_predictions, every_10_frame_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_predictions(predictions):\n",
    "    ''' Post processes the predictions we receive to make comparisons/results easier to read. '''\n",
    "    model_preds = {}\n",
    "    for model in predictions:\n",
    "        video_id_preds = []\n",
    "        for video_link in predictions[model].keys():\n",
    "            video_id = video_link.split(\"/\")[-1] # We only want the last part of the path\n",
    "            video_id_preds.append(video_id)\n",
    "        model_preds[model] = video_id_preds\n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Results From the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for query_image_path in query_image_paths:\n",
    "    results[query_image_path] = {}\n",
    "    results[query_image_path] = post_process_predictions(query_image(query_image_path, models, len(ground_truth_ids)))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the final evaluation results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the evaluation results, computing the number of matches between my ground truth and the models' predictions\n",
    "\n",
    "matches = {}\n",
    "for query_image_path in query_image_paths:\n",
    "    curr_results = results[query_image_path]\n",
    "    matches[query_image_path] = {}\n",
    "    for model in results[query_image_path]:\n",
    "        num_matches = []\n",
    "        for index, id in enumerate(ground_truth_ids):\n",
    "            if id in results[query_image_path][model]:\n",
    "                num_matches.append(index+1)\n",
    "        matches[query_image_path][model] = num_matches\n",
    "matches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d832aa6a45f3eb7e7bad55fdbd58c33e34218938f08b2d0a8c371f719f367be4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

% !TEX TS-program = pdflatex
\documentclass[10pt,twocolumn]{article} 

% required packages for Oxy Comps style
\usepackage{oxycomps} % the main oxycomps style file
\usepackage{times} % use Times as the default font
\usepackage[style=numeric,sorting=nyt]{biblatex} % format the bibliography nicely 

\usepackage{amsfonts} % provides many math symbols/fonts
\usepackage{listings} % provides the lstlisting environment
\usepackage{amssymb} % provides many math symbols/fonts
\usepackage{graphicx} % allows insertion of grpahics
\usepackage{hyperref} % creates links within the page and to URLs
\usepackage{url} % formats URLs properly
\usepackage{verbatim} % provides the comment environment
\usepackage{xpatch} % used to patch \textcite

\graphicspath{ {./images/} }

\bibliography{refs.bib}
\DeclareNameAlias{default}{last-first}

\xpatchbibmacro{textcite}
  {\printnames{labelname}}
  {\printnames{labelname} (\printfield{year})}
  {}
  {}

\pdfinfo{
    /Title (YouTube Image Search: Is It Viable?)
    /Author (Christopher Linscott)
}

\title{YouTube Image Search: Is It Viable?}

\author{Christopher Linscott}
\affiliation{Occidental College}
\email{clinscott@oxy.edu}

\begin{document}

\maketitle

% Refer to rubic: https://docs.google.com/document/d/14caXj4RZ4pBAFtMEgXHtxMqi6gLjo8MnY1RSmGvkUgo/edit

\section{Problem Context}

% Furthermore, when learning new topics, one may come across a topic or object they’ve never seen before. Getting more information feels paradoxical, as obtaining further knowledge given only visual context is difficult; in a sense, you “don’t know what you don’t know”. 

What does the phrase "football coach" mean? What breed of dog is in this photo? Text-based search has been widely used since the 1990s \cite{}, but many issues have arised with it. Depending on where one is from, what dialect they use, words and phrases people use can have ambiguous meaning \cite{Beall2008}. If they cannot describe what they're searching for, getting information can feel paradoxical, as forming a search query given only visual context is difficult; you "don't know what you don't know." Difficulties in the ambiguity of language and the imprecise medium to describe what we're looking for, lead to more false positives, more time spent searching for the information within the results, and worst of all leads to more questions unanswered \cite{}.

Current video recommendation methods, like YouTube, rely on textual information about the video using keywords \cite{Stanford2021}. Without knowing the name or a related keyword, one cannot simply search the web in hopes for an answer. Not only does image search help users tell the search engine what they’re looking for (with less ambiguity), but image-related search removes the need to annotate images online with keywords as image similarity can be used as a heuristic \cite{Adrakatti2016}.

Therefore, my solution emulates image search in YouTube, as a means to solve this information gap using related videos, while also offering times within videos where the relevant content is found. A user inserts an image of an object in real life or even a drawing (from their lecture notes for example), and my project returns videos with related images either from the thumbnail or the video frames. If related video frames are found within the video, times corresponding to these frames will additionally be returned.

By using images, there is far less ambiguity and taking a picture of something unknown is far easier than describing it. Therefore, my solution solves these problems of text-based search, and speeds up the searching process by including relevant times within videos, so users don't always have to sift through videos for relevant information.

% From an educational perspective, not only does this help students fulfill these information gaps through videos, but it can speed up the studying process as users don’t always have to sift through a video to grab the relevant information.

\section{Technical Background} 

YouTube videos can be broken down into images through both the thumbnail, an image preview of the video content, and the individual frames which make up the same video. Determining a description of an image means taking into account the positions of objects, edges, shapes, and other characteristics of objects that may render useful. Mathematical "descriptions" of images are created, containing numerical information of visual characteristics (called features), which can be compared to each other using distance-based or object-based methods. Upon the insertion of a query image, a description of the query image will be computed and will be compared against stored descriptions of other images (whether frames or thumbnails) to determine related videos.

Creating these "descriptions" of images required using a Convolutional Neural Network (CNN), a specialized Artifical Neural Network designed for pattern recognition. A more semantic description relating to the objects in the image, was created utilizing a Faster-RCNN (RCNN), or Region-based Convolutional Neural Network. The description of these images will look like vectors and maps respectively, the CNN vectors containing number which represent the "activation" or how much particular features (such as shape, texture, or color) exists within different regions of a given image. The RCNN will return JSON, or a map which for every object class it sees ("dog"), tells the number of those objects it sees in the image (5).

Upon inserting an image, finding relevant information means finding images (that refer back to a given video) which relate to our input image by comparing these features. By utilizing video frames in addition to the thumbnails, we can determine the time within the video which is relevant, from the number of frame, as we grab one frame every second.

With a Convolutional Neural Network, we have 3 important layers which are the Convolutional Layer, Pooling Layer, and Activation Layer. While the Fully Connected Network is important in most neural networks, as we want the low-level image representation and not the high-level classification the model makes, we remove this layer. Convolution refers to the combination of two functions, and is often used in Probability Distributions; in the case of our CNN, the convolutional combines the input information (in the form of raw pixel values) with the values of a sliding filter, merging them together. The filters that we choose, or are pre-trained normally, will affect the image in different ways in order to highlight different characteristics of the image. 

Suppose we have a matrix F of raw pixels and our filter matrix G. To compute the convolution or merge the raw pixels and filter matrix G together, we compute the following grid square for our output feature map $H$ at a row $m$ and column $n$:

\[ H(m, n)  =  (F * G)(m, n) = \sum_{0}^{j} \sum_{0}^{k} G(j, k) * F() \]

Depending on parameters of stride, kernel size, and padding around the image, we get a output feature map which represents the merging of the image and filter. This becomes important, depending on the filter we choose. For example, the Sobel Filter \cite{} acts as a edge detector, where different matrices or kernels can detect different edges. So, when our image is passed through this layer, it essentially highlights the edges of the image, which I'll describe as "activations" of that feature being an edge. With these activations of these features, the CNN can learn information such as edges, textures, colors, and other characterisitics and combine them to make decisions on what features and/or objects it sees. Overtime, through passing these features further into convolutional layers, the model can take lower-level features (such as edges) and begin to combine and get activations of higher-level features (such as nose, head, eyes) until it has a very semantic representation of the image

The unsung hero of the CNN is the pooling layer. The pooling layer acts as a feature "summarizer", where it grabs the maximum value of a group (2x2, 3x3) of grid squares (or pixels) and uses this as one grid square of the output. In a similar fashion to the kernel in the convolutional layer, the pooling layer will be simply "slide" over the image and compute the maximum of the activations of features it sees. There are several things this layer does: it reduces the size of the input overtime, reducing it to something more readable (highlighting where and how much a feature is present), reduces the overall computation (since with our deeper levels, we only look at these summarized activations as opposed to the entire image), and it can tells us spatial information (which is important in the context of an image) which helps the model perform well in conditions where the image is translated (moved side to side) or rotated as the positions of features will change (but the activations will not).

Finally, we have the activation layer. Depending on the choice of activitation, the model will either highlight the maxima and/or minima. As many models have had success with the ReLU activation layer \cite{}, it is the favorite and the Xception model includes this layer as well. It will alter the values of the outputs of the convolutional and pooling layers, by either storing a value of 0 or the value (if the value is higher than 0). The activation layer essentially captures what features exists, as opposed to unneccesaary information (such as where features don't exist) \cite{}. By doing so, we summarize the image features further for the next layer.

Moving on, the Faster R-CNN is a third-generation object detection model, sprouting from the original R-CNN. The Faster R-CNN utilizes a Region Proposal Network (RPN) and convolutional layers (as discussed above) to 

\section{Prior Work}

Many image-related applications have been developed around gathering information. A popular application is Google Lens, an application which in real time can analyze an image (whether it has text or a given object) and identify the object or text. Furthermore, image-related search applications, such as Google’s reverse image search, are very useful in determining the source of an image or determining similar images based on an input image. In fact, it’s being used often for educational purposes such as with plant identification \cite{Moore2018}. In a similar way to how Google Lens and Google’s reverse image search aims to solve the information gap (given only visual context), my application serves to solve it using related videos as opposed to a purely trained AI model on the classification of objects.

To go on, researchers at Stanford have utilized video frames within a given video (along with labeled good and bad thumbnails) to train a convolutional neural network to determine what frames of that video would constitute as a good “thumbnail”, or image preview to capture a user’s attention \cite{Stanford2017}. Extending this idea, we can utilize the video frames in order to gather images to act as “markers” for the videos; when an user-given image matches, it references the video and/or the time within it.

Along with classifying YouTube thumbnails together, neural networks have also proven to be able to learn to cluster YouTube thumbnails together, categorizing them into different categories without the help of tags (i.e. labeled categories on the videos). Utilizing k-means clustering, they were able to compare and determine that a clustering algorithm could categorize videos by their thumbnails as well as YouTube did manually with tags \cite{Stanford2021}. As my dataset is not labeled and neural networks have shown to categorize based on features as well (with clustering), likewise I'll extend this idea to segment images and develop the clusters/categories in an unsupervised manner with a similar combination of neural network (CNN) and clustering method (FCM).

A final investigation of other solved problems led me to Kavitha's paper, which utilizes and recommends the usage of Fuzzy C Means for image segmentation as it has higher precision and efficiency; Kavitha utilizes FCM to gather information from image for natural disaster management \cite{Kavitha2020}. Not only do I need to group images based on image visuals, but I need an efficient method to perform it (as I only have access to Oxy's computer and my own). As a clear parallel, I can utilize Fuzzy C Means as my clustering algorithm as opposed to K-Means or other clustering methods.

% However, there are many ways (or algorithms) to cluster a dataset, depending on the type of data one is handling. Therefore, analyzing papers which tackled a similar problem led me to Kavitha’s paper, which retrieves images that are relevant to the user given image, for the purposes of getting information that’s useful for natural disaster management. The paper pushes that other image-retrieval algorithms are incomponent in terms of efficiency with clustering images, and that utilizing the Fuzzy characteristic algorithm can achieve great precision with better efficiency \cite{Kavitha2020}. Furthermore, an article about the many methods/algorithms of clustering agreed that Fuzzy algorithms are best for image segmentation which aligns with the arguments of the paper \cite{PrasadClustering}. As my project will involve deriving related images and image segmentation (in order to make the images more meaningful), my project landed on the Fuzzy C-Means Algorithm (as opposed to a K-means clustering) as the resource for clustering my images all together and creating these baskets.

\section {Methods}

Data was fetched from Google's Youtube8M Dataset, containing data in the form of tensorflow record files (*.tfrecord). These tensorflow records contained high-level features of the video (such as cateogry and a general description), and a fake id related to the YouTube video. So, using tensorflow's parser, I parsed the tensorflow record files, obtaining the fake id assigned to a video and the video's human-labeled features or overall characteristics. With the fake id, I could grab the real YouTube ID assigned to the video through a link provided by Google, and download the video by using a python module called yt-dlp. As not only grabbing the YouTube ID and corresponding YouTube video took large amounts of time, I asynchrously fetched the YouTube IDs and upon arrival (or the completion of the HTTP request), I used multithreadding to concurrently run the downloads of the YouTube videos. After obtaining the thumbnail and the corresponding video frames, I used cv2's imread and resize function to convert the thumbnail into an 3D array of width 299 pixels, height 299 pixels, and 3 channels representing the Red, Green, and Blue values to match the input expected by the Convolutional Neural Network. 

As the data consisted of images, and the image often had real-life subjects, I utilized the Convolutional Neural Network which could learn and describe characteristics of objects and regions of images. As the YouTube images are unlabeled, I wanted a CNN model that was pre-trained on images, so I didn't have to do this training myself (which would mean more data and time). A final point is that I wanted the model to be efficient, but thorough in its computation. With these benefits in mind, the Xception model was a clear choice, as I could create a pre-trained version of the model (with images from ImageNet) utilizing the keras library in Python. As well as, compared to other pre-trained (on ImageNet) models available, the Xception model was very efficient, while maintaining a very high top-five accuracy for image classification, which is important in the context of related videos (where finding many good options can be better than finding only one great option) \cite{Stancic2022}. Efficiency becomes important when computing millions of images to decrease the amount of computing power and time for the image. A Convolutional Neural Network, in its normal state, will return what objects it believes it sees (with Image Classification), but this is far too semantic and researchers found that removing the fully connected layers of the model improved the accuracy and ability to compare images based on similarity. Therefore, to construct the AlteredXception (I will call it), the Xception model was first created, and using the keras model, I removed the final layers of the Xception model, and made the final output the Pooling Layer. By having the final output being the Pooling Layer, the final output will consist of the flattened feature maps which tells us what regions have specific characteristics.

After utilizing this library, I began using the model to gather representations of various images from frames and thumbnails I gathered, passing them through the 


 
While the AlteredXception model is an excellent model in deriving features from an image, one problem is that it may see the image is one way. The max-pooling and average-pooling layers of a Convolutional Neural Network will spatially group and based on activations of neurons together, will determine features for several parts of the image. However, we could look at the image in different ways using the RMAC or Regional Maximal Activations of Convolutions. By precomputing different square regions to look over, as opposed to grabbing features and computing the vector in one way, we compute many different vectors based on different sized regions (to grab small and big features) and combine them together to form a vector which captures many different view points. Not only is this just as efficient as the normal CNN (as we just use the feature maps from the neural network), but this algorithm has proven to perform better than normal standard CNNs in image classification \cite{}.

To store the image representations the models computed in the image, I utilized my file system as a database, consisting of only JSON and NPY files. JSON files were initially used as they allowed me to maintain order and also check for duplicate predictions in my database. By storing the image paths as a key and the predictions as a value, I can simply check if an image path already exists within my JSON before computing the prediction (and wasting time creating duplicate predictions). As well as, I can use it to order my image paths and view it relatively easily. However, a big problem with JSON files is that loading the files take a long time, which in the context of an user interface and computing other predictions, can mean long wait times (on the magnitude of minutes) to load and have the prediction maps ready to be queried from or added to. So, based on research online \cite{} and the overall ease-of-use with the numpy library (as my image representations were large numpy arrays), .npy files were used to store the image representations. To maintain the benefits of JSON with duplicates, I created a .npy file for the image representations (essentially a giant array of numpy arrays), and a JSON file which mapped an image path to the index of where the corresponding image representation is within the numpy array. So, the numpy files are faster to load and store, are smaller in size (as numpy has methods to efficiently store their arrays in npy files), and we still gain the benefits of JSON! For the Object Detection Model (unlike the other models), inverted indexing could be utilized for faster querying. Inverted Indexing is the process of storing "words" to image as a way to determine and find more related images quickly. If our query image has visual words computed, we can use them to find other related images based on matching object classes. In the case of the Faster-R-CNN model, images had representations consisting of the objects' classes, sizes, and the model's confidence scores. Therefore, we can use inverted indexing to store an object class and map it to image paths which had the same object class. Each image path would also map to its confidence score of that object classes, so when we query, we can favor images where the model was more confident. By using this style of "database", when we compute the objects of an image, we can use the objects we find to query for images (and therefore videos) with related objects very quickly (since we don't look at all images, only those which match).

In combination with the XceptionModel, a pre-trained Object Detection Model, called the Faster R-CNN model, was utilized for its accuracy and ease-of-use with general object classes. While it couldn't pick up on nuances very well, it can be used as a discriminator as well as for grabbing relevant (but not specific) videos very easily. 

After attaining the descriptions of our YouTube images through these models and storing them away, the description of the query image will be computed using the same methods. To compare the descriptions created by the CNNs, I utilized Euclidean distance, or the sqaure root of the sum of the squared differences between the vectors. With this distance, we are essentially computing the difference of features within regions of the image. If there are more regions with different characteristics between images, they are viewed as farther in distance. If regions has similar characteristics in many regions, they were viewed as closer together. For descriptions created by the Object Detection Model, I computed a score based on the number of matching object classes and the difference between the number of objects (per class) in the query image and the number of objects in the database image. With similar images, they should have the same number of matching objects (images with two dogs should return images with two dogs). As well as, the number of matching classes matter (images with a dog and bike should return images with a dog and bike and not just dogs or bikes). By computing these differences, we return the relevant videos based on a high object score or a low Euclidean distance. 

With the comparison in place, we have a way to get related videos form the query video. To make and visualize this system, I utilized flask, a Python library, which can quickly make a web server to serve HTML, CSS, and JS files to show the relevant videos and provide an input to take in query images. Utilizing HTML and CSS, I created a base website which would take the query image and send a POST request to my web server with the image. With the query image from the user, I would compute the image and return the relevant videos for each model in the form of JSON. Utilizing Javascript, I created iframes and section to store each individual video and individual times (which became clickable buttons to go to that time of the video) for each video. Not only does this user interface showcase the models and database, but it provides a preview into the benefits of image search by including things like relevant times. 

\section{Evaluation Metrics}


While creating this image search tool, there were three questions asked about: the model's ability to find specific, relevant videos (such as a golden retriever versus a english bulldog), the model's performance under various conditions, and most important how the model would perform under a small or large number of videos. 

In the context of any search tool, an important metric is similarity and the ability to pick up nuances and return relevant and specific images, either in the context of a small or large number of possibilities. As an example, if we submit an image of a golden retriever into a dog database, we should expect the model to return golden retriever videos, and not only just dog videos.

To test the evaluation of this image search tool, I tested common conditions where upon a given query image, the difference between what we predicted and what the model predicted.

To test the model, we utilized data collection, data labeling, and determined query images. For dog, game, and art videos, I downloaded one hundred random videos from YouTube8M's dataset of that corresponding category, downloading the frames and thumbnails as well. As content-based image retrieval doesn't have a common method of obtaining "relevant judgements" for a query \cite{Muller2001 TODO}, I determined, with my own judgement, descriptions of these videos and a suitable query image that had many possible relevant videos. With this query image, the top ten most relevant videos were picked based on the same judgement, and this will be referred to as my ground truth in evaluation. After having the models compute the image representations of the frames and thumbnail, the query images were queried inside these small databases, and the top 10 videos were returned from the model. To determine how well the model performed, we compared and determined how many of the top 10 videos the model predicted matched the ground truth "videos" I had picked previously. Based on the models' scores, we can determine which model and/or algorithm (such as the normal CNN versus the RMAC algorithm), performed better. While other methods such as precision-recall (PR) Graphs \cite{Muller2001 TODO} or more human evaluators could have been utilized, based on the scope and time for this project, comparing the results for matches in relevance was a much simpler and more time-effective method for evaluating the models.

While placing in normal images is a great metric, YouTube videos, with the goal of attracting or keeping viewers, often will not have just plain images as their videos' thumbnail nor frames, but will have text or have visuals (such as red arrows) to direct and grab the users' attention. Often, content creators will utilize techniques such as curiosity gaps and manipulation of the images with text and visuals \cite{Mowar2021}. So, to account for this image manipulation, distorted and "clickbait" query images (by adding text and red arrows to the image in likely positions) were also utilized. To distort the images, \href{https://www.imgonline.com.ua/eng/picture-distortion.php}{a simple online image distorter} was used with an image distortion intensity of 10. To add "clickbait" to the images, \href{https://www.visualwatermark.com/add-text-to-photos/}{a different online tool was used}, where hyperbole text, such as "Golden Retriever Goes Animal", relating the image and red arrows was added to the image.

As a final metric, I wanted to test how the model would perform under a large and small, specific database. After pushing the videos through the category-based database, I pushed them through a larger database filled with one hundered thousand videos, including ones from the categories being tested over. YouTube’s 8M Dataset, which contains 6.1 million videos from 2017 and 2018 \cite{googleYT8M}, has fewer videos compared to the estimated 800 million on Youtube \cite{EarthWeb TODO}. By comparing to see the results on a slighly larger number of videos, we can use it as a measure of whether the models and database implementation (in their current state) could be used for something as large as YouTube.

Once the different query images were pushed through, the results were returned and placed inside of a Google Sheets spredsheet for ease of use and storing. The graphs were plotted to compare the results under the different conditions and to determine which models performed better or worse under which categories or conditions.


As a baseline, to determine if the image search could act as a "reverse video search", frames of videos were passed through the model, to determine if the corresponding video (with the frame) would show, in both small and large database context. If the model wasn't a good image search, it may be a worthwhile reverse video search tool, which can still be useful in information gaps.
% Other conditions such as "reverse image search" and drawings were tested as well. To determine if the model can return the video based on one of its own frames, an individual frame was picked from the video and passed through to see if the corresponding video would show (both in the small and large database context). 

\section{Results and Discussion}

My solution, in its current state, is not viable. With small databases of one hundred category-related (games, art, dogs) videos, the convolutional neural network (CNN, CNN + RMAC) and object detection model (RCNN) performed very well, capable of grabbing up to four relevant picks (out of a hundred other relevant videos) from a non-distorted query image. When the model found a relevant video, 70\% of the time it was a top-5 video or better in regards to my own ground truth.
Under conditions such as image manipulation with text, visuals, or distortion, the models did not suffer, losing one to two top-10 picks; the adapated CNN was consistent across the query images (distorted, or added text and visuals) given to it. As a final note for the small-scale test, the models were able to reverse-image search the videos from the frames given it. 

However, the models had poor performance across the board with a large-scale database, having one hundred videos. Across all models and categories, only the adapated CNN (with RMAC) model was able to determine one top-10 pick. Furthermore, the models had issues either in prediction speed and the prediction (image characteristics) representation resulted in a slower querying time. With the Convolutional Neural Network, the computation of the image "description" was quick, but the reperesentation and query implementation (Euclidean distance) results in a slow query time of three to four seconds in large databases of one hundred thousand videos. However, with the Object Detection Model, the querying speed was very quick with a hundred tenths of a second for the large database, but the computation of the image objects was extremely slow at around five to ten seconds. Because the accuracy and speed of these models (either in computation or querying) is more than a few seconds, it's not viable in the context of a search tool used on the Internet, where fractions of a second count \cite{GOOGLE TODO}.

The first problem revolves around the prediction accuracy. The image predictions come in the form of a two thousand dimensional vector. The original functon of Euclidean distance is to compute the difference in characterisitics (i.e. computing the squared differences between dimensions of the vector) between images, discrimating by favoring images which have a small difference of characteristics. A big issue with the Euclidean distance metric revolves around the Curse Of Dimensionality, where the distance between points in a high-dimensional space approach some constant \cite{WIKIPEDIA TODO}; all videos and images are equidistant and therefore will return the same similarity (distance). Therefore, the Euclidean "loses its function" \cite{WIKIPEDIA TODO} of discriminating or computing the distance between these images and non-relevant images/videos will be returned. With a small number of videos, as there is a small number of data points, the points are likely to be sparse and distant enough to be meaningful (to determine similar, relevant videos) in the high-dimensional space. However, when you have a high number of videos, the flaws of this distance metric are exacerbated, where non-relevant videos/images and relevant videos/images are likely to be equidistant, reducing the accuracy (or number of top-10 picks) from this distance metric.

The second problem revolves around the querying speed. To determine the most similar images, we need to compute the distance between all the images present in the database. While computing the distance is fast, the magnitude of videos results in a slow query time for the Convolutional Neural Network. The final small problem revolves around the prediction (or image computation) speed for models, such as the Object Detection Model (RCNN). While the querying speed is quick, as the Object Detection Model needs to propose regions, compute the characteristics, and determine the objects of the image, depending on the size of the image, the model can take 5-10 seconds which is too long (for similar reasons).

Due to the architecture of the image representations (in database), query methods, and the model prediction speeds, the search tool is far too slow with lots of videos and is not viable for a large-scale video platform like YouTube.

While this project is not currently viable, the prediction accuracy and speed can be improved. To begin, for the Convolutional Neural Networks, inverted indexing could be utilized. Already present within the Object Detection Model, it utilizes a map of "visual words" (in this case, vectors of a smaller dimension representing some characteristic like a dog ear) to images with this visual word. As opposed to querying every image in our database, querying involves only computing with images which have the same visual words. Not only will this increase the speed, but by storing and querying based on these "visual words", we can discrimate more effectively by choosing images with the same visual words as our query image. Other ways to increase the accuracy without changing the models or databases may be utilizing an ensemble model, such as a decision tree, which learns (through some labeled data) to weigh the outputs of the Convolutional Neural Network and Object Detection Model and determine a better-combined set of relevant videos based on the Convolutional Neural Network seeking difference in characterisitics and the Object Detection Model acting as a baseline discriminator. Other methods of computing similarity such as Manhattan Distance or utilizing clusters could be utilized to increase accuracy with the high-dimensional data.

With these improvements to the Convolutional Neural Network, we can increase the accuracy and speed of the model to perform better in large-scale database and make it more viable.

\section {Ethical Considerations}

% - less extreme on proposal of ethics
% - that are most relevant 
% - dw about image manipulation
% - (dias)selection bias
% - warning based on keywords of videos
% 	- human segmentations?

% The project is considered both in its complete technological and societal context. 
% Issues of bias and diversity are explored in detail, and potential contributions to global and local inequity examined. 
% Relevant literature is cited.

In consideration of the ethical aspects of this project, there are aspects of the data and algorithms which have bias including the Youtube 8M Dataset and clustering algorithm.

% In the process of creating this project, there are ethical concerns of this project such as the bias within the data used for the CNN and clustering, as well as, a lack of explainability and transparency entirely behind my algorithm.

To begin with, the YouTube 8M Dataset primarily contains selection bias and sample bias due to the limited amount of data and diversity surrounding video platforms and YouTube 8M’s dataset. Selection bias, to be general, is the difference in characteristics between those selected for a study (training data) and those not (test / real world data) \cite{Yu2020}; YouTube’s 8M Dataset, which contains 6.1 million videos from 2017 and 2018 \cite{googleYT8M}, has much less data compared to the estimated 800 million on Youtube. While the dataset contains the data across the same amount of general categories (15), the distribution of data across these categories and subcategories is weaker, primarily having videos in the games or vehicle category \cite{googleYT8M}. This combination of lack of data and diversity within the data contributes to poor generalization, or poor performance on the test set (real world) in comparison to the training set, due to the model overfitting to the training data and struggling to account for new characteristics from new input images \cite{Yu2020}. The main reason behind poor generalization in selection bias comes from unwanted correlations between features or characteristics in images; clustering on images with these correlated features lead to “spurious (fake) relationships” \cite{Yu2020}. Furthermore, due to the selection of data by YouTube, the YouTube dataset has sample bias and a favoring toward Western ideals. To reiterate, the videos from the dataset are primarily distributed in games, vehicles, and concerts with around 780,000 videos, whereas only 3000 videos are available about Indian cuisine and TV. The major advertising audience from YouTube is aged 25-34, male, and from India \cite{HootSuite2022}; despite this, there is a disparity in content and data surrounding India. Part of the reason behind the creation of the dataset was addressing the need for “large-scale, labeled datasets” and “[accelerating] research on large scale video understanding” \cite{Warrick2020}. As gaming and car content thrives on YouTube, this data may’ve been curated to not only appeal to Western developers, but to bring research on the largest sources of videos. Additionally, as the data is human-labeled \cite{googleYT8M}, only those from Google who had the technology and expertise to label the data did so. In the process, this disfavors content and people (who would label) from other backgrounds and lessens the diversity of the data. This selection of data affects the clustering algorithm in a similar way to sample bias, or bias due to nonrandom data. Like selection bias, sample bias will cause correlation between features; as non-popular videos (due to language or cultural barriers) are less likely picked (and have small groups), it’s less likely the clustering algorithm will create a separate cluster for them. In the process, the algorithm is less likely to group objects and people from lesser-known videos/backgrounds together, instead placing them in a larger cluster (from some other identifying feature) with less related videos.

As a final point, due to the lack of explainability behind the machine learning algorithm’s decisions, finding and fixing the sources of unethicality (bias) can become an ethical issue. While many machine learning algorithms, Fuzzy C-Means included, are mathematically explainable in how they converge or determine clusters, these explanations don’t describe why they decided on a specific set of clusters. As a consequence of this implementation, fixing a known source of bias and error, through fixes in the implementation, is difficult; telling the computer where the centers should be removes the machine learning aspect of this project. As well as, the high number of dimensions (\(>100\)) I'll be clustering over may become hard to conceptualize, understand, and explain. Therefore, unwanted biases may occur in the results of my project (and have simply make the biases previously stated worse), with no guarantee of being able to fix them before or after deployment. 

% Selection bias, to be general, is the difference in characteristics between those selected for a study (training data) and those not (test / real world data) \cite{Yu2020}; it primarily occurs due to an underrepresentation of the population from the sample.  While the dataset contains the data across the same amount of general categories (15), the distribution of data across these categories and subcategories is weaker, primarily having videos in the games or vehicle category \cite{googleYT8M}. While YouTube has the same number of categories, they are meant to encompass many of the videos together; the subcategories (and more niche categories) of YouTube beneath these general categories are not shown or displayed (more inferred and created by subcommunities). This combination of lack of data and diversity within the data contributes to poor generalization, or poor performance on the test set (real world) in comparison to the training set, due to the model overfitting to the training data and struggling to account for new characteristics from new input images \cite{Yu2020}. The main reason behind poor generalization in selection bias comes from unwanted correlations between features or characteristics in images; clustering on images with these correlated features lead to “spurious (fake) relationships” \cite{Yu2020}. A great example is working with an image dataset containing images of mainly dogs laying in grass. Upon the introduction of an image of a cat laying in grass, due to the high correlation between dogs and grass (implicitly in the data), the cat gets placed in the wrong cluster with the dogs \cite{Wang2020}. In a similar fashion, any unintentional and intentional biases of the YouTube 8M dataset will be exacerbated, by the model conforming to what it believes is the population (training data) and forming relationships (such as between dogs and grass) about this data which don’t exist on all of YouTube. The subtypes of selection bias, historical and sample bias, can further describe the specific limitations and contribution effects of bias in our data and why our data cannot be ethical when used for machine learning.

% Finally, due to the selection of data by YouTube, the YouTube dataset has sample bias and a favoring toward Western ideals. To reiterate, the videos from the dataset are primarily distributed in games, vehicles, and concerts with around 780,000 videos, whereas only 3000 videos are available about Indian cuisine and TV. The major advertising audience from YouTube is aged 25-34, male, and from India \cite{HootSuite2022}; despite this, there is a disparity in content and data surrounding India. Part of the reason behind the creation of the dataset was addressing the need for “large-scale, labeled datasets” and “[accelerating] research on large scale video understanding” \cite{Warrick2020}. As gaming and car content thrives on YouTube, this data may’ve been curated to not only appeal to Western developers, but to bring research on the largest sources of videos. Additionally, as the data is human-labeled \cite{googleYT8M}, only those from Google who had the technology and expertise to label the data did so. In the process, this disfavors content and people (who would label) from other backgrounds and lessens the diversity of the data. This selection of data affects the clustering algorithm in a similar way to sample bias, or bias due to nonrandom data. Like selection bias, sample bias will cause correlation between features; as non-popular videos (due to language or cultural barriers) are less likely picked (and have small groups), it’s less likely the clustering algorithm will create a separate cluster for them. In the process, the algorithm is less likely to group objects and people from lesser-known videos/backgrounds together, instead placing them in a larger cluster (from some other identifying feature) with less related videos. Additionally, the algorithm may choose to focus on a well-known feature of a lesser-known video as opposed to a lesser-known, identifying feature of the video or thumbnail. As a result, finding related videos for an object or person not recognized in the United States becomes more difficult (and may even require parsing through the relevant videos by the user which defeats the purpose). Evidently, the unintentional biases present within my data contribute to the algorithm making incorrect relationships in the data and unethical decisions (clusters) based on its interpretation of the population.

% As a final point, due to the lack of explainability behind the machine learning algorithm’s decisions, finding and fixing the sources of unethicality (bias) become too tedious and difficult to handle. While many machine learning algorithms, Fuzzy C-Means included, are mathematically explainable in how they converge or determine clusters, these explanations don’t describe why they decided on a specific set of clusters. For the Fuzzy C-Means algorithm, as the initial centers of centroids are picked at random and the placement of these initial centers affects the end result, there’s no way to backtrack mathematically. The algorithm can perform very well or very poorly, and the best method of overcoming this simply involves recomputing the clusters until the error (or distance between the cluster’s centroid and its corresponding distance) is minimized \cite{GeeksForGeeks2019}. As a consequence of this implementation, fixing a known source of bias and error, through fixes in the implementation, is difficult; telling the computer where the centers should be removes the machine learning aspect of this project. As well as, if and when my algorithm acts unfairly, I cannot determine the source of its decision whether it’s due to the data or implementation of my algorithm. While I can reiterate and determine a relatively good set of clusters (using methods like the Elbow method to evaluate them \cite{PrasadClustering}), this doesn’t ensure optimal or unbiased results. This translates to deployment where the algorithm may perform horribly and under new data, return unexpected results without any sort of way to determine where it went wrong. The main solution to this problem is either removing the root cause (if it’s findable), or removing the project from deployment immediately (like with Microsoft’s Zo). As finding the root cause of an unethical decision/result is difficult, neither of these decisions are sufficient for completing and deploying my project. Either I have to micromanage the algorithm (which defeats the purpose of it being machine learning), or I have to start from scratch with the data and algorithm (which already has presented problems). In total, unintentional and intentional data biases present themselves as unavoidable and difficult ethical issues to fix; finding and fixing all of these is not doable given the scope of my project.

\printbibliography
\end{document}
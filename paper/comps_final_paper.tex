% !TEX TS-program = pdflatex
\documentclass[10pt,twocolumn]{article} 

% required packages for Oxy Comps style
\usepackage{oxycomps} % the main oxycomps style file
\usepackage{times} % use Times as the default font
\usepackage[style=numeric,sorting=nyt]{biblatex} % format the bibliography nicely 

\usepackage{amsfonts} % provides many math symbols/fonts
\usepackage{listings} % provides the lstlisting environment
\usepackage{amssymb} % provides many math symbols/fonts
\usepackage{graphicx} % allows insertion of grpahics
\usepackage{hyperref} % creates links within the page and to URLs
\usepackage{url} % formats URLs properly
\usepackage{verbatim} % provides the comment environment
\usepackage{xpatch} % used to patch \textcite

\graphicspath{ {./images/} }

\bibliography{refs.bib}
\DeclareNameAlias{default}{last-first}

\xpatchbibmacro{textcite}
  {\printnames{labelname}}
  {\printnames{labelname} (\printfield{year})}
  {}
  {}

\pdfinfo{
    /Title (YouTube Image Search: Is It Viable?)
    /Author (Christopher Linscott)
}

\title{YouTube Image Search: Is It Viable?}

\author{Christopher Linscott}
\affiliation{Occidental College}
\email{clinscott@oxy.edu}

\begin{document}

\maketitle

% Refer to rubic: https://docs.google.com/document/d/1oiXngqxh30ADXVPfOEnNuBNX1DGFmmExI6DoGZNdrs0/edit

\section{Problem Context}
% Whether you're American or British, you may interpret these statements differently. \cite{} However, if I give you a picture of a 


% Furthermore, when learning new topics, one may come across a topic or object they’ve never seen before. Getting more information feels paradoxical, as obtaining further knowledge given only visual context is difficult; in a sense, you “don’t know what you don’t know”. 
% In order to determine information about an image and get a corresponding datapoint, in processing images and assigning them a belongingness metric, By doing so, we can retrieve an "activation" or how much of a particular feature is in a given image. Thereby, we can determine the activation of surroundings, noise, objects, and people in a given image. By assessing these visual characteristics, we can derive a datapoint for a particular image, and utilize the function and algorithms previously discussed. By utilizing these activations, we can flatten them into one long vector and insert them as one single datapoint of dimension d. By clustering, using the Fuzzy C Means algorithm, and gathering datapoints, using a CNN, we'll have representatives baskets of images that relate to each other based on features derived from the CNN. 

What does the phrase "football coach" mean? What breed of dog is in this photo? Text-based search has been widely used since the 1990s \cite{}, but many issues have arised with it. Depending on where one is from, what dialect they use, words and phrases people use can have ambiguous meaning \cite{Beall2008}. If they cannot describe what they're searching for, getting information can feel paradoxical, as forming a search query given only visual context is difficult; you "don't know what you don't know." Difficulties in the ambiguity of language and the imprecise medium to describe what we're looking for, lead to more false positives, more time spent searching for the information within the results, and worst of all leads to more questions unanswered \cite{}.

Current video recommendation methods, like YouTube, rely on textual information about the video using keywords \cite{Stanford2021}. Without knowing the name or a related keyword, one cannot simply search the web in hopes for an answer. Not only does image search help users tell the search engine what they’re looking for (with less ambiguity), but image-related search removes the need to annotate images online with keywords as image similarity can be used as a heuristic \cite{Adrakatti2016}.

Therefore, my solution emulates image search in YouTube, as a means to solve this information gap using related videos, while also offering times within videos where the relevant content is found. A user inserts an image of an object in real life or even a drawing (from their lecture notes for example), and my project returns videos with related images either from the thumbnail or the video frames. If related video frames are found within the video, times corresponding to these frames will additionally be returned.

By using images, there is far less ambiguity and taking a picture of something unknown is far easier than describing it. Therefore, my solution solves these problems of text-based search, and speeds up the searching process by including relevant times within videos, so users don't always have to sift through videos for relevant information.

% From an educational perspective, not only does this help students fulfill these information gaps through videos, but it can speed up the studying process as users don’t always have to sift through a video to grab the relevant information.

\section{Technical Background} 

YouTube videos can be broken down into images through both the thumbnail, an image preview of the video content, and the individual frames which make up the same video. Determining a description of an image means taking into account the positions of objects, edges, shapes, and other characteristics of objects that may render useful. Mathematical "descriptions" of images are created, containing numerical information of visual characteristics (called features), which can be compared to each other using distance-based or object-based methods. Upon the insertion of a query image, a description of the query image will be computed and will be compared against stored descriptions of other images (whether frames or thumbnails) to determine related videos.

Creating these "descriptions" of images required using a Convolutional Neural Network (CNN), a specialized Artifical Neural Network designed for pattern recognition. A more semantic description relating to the objects in the image, was created utilizing a Faster-RCNN (RCNN), or Region-based Convolutional Neural Network. The description of these images will look like vectors and maps respectively, the CNN vectors containing number which represent the "activation" or how much particular features (such as shape, texture, or color) exists within different regions of a given image. The RCNN will return JSON, or a map which for every object class it sees ("dog"), tells the number of those objects it sees in the image (5).

Upon inserting an image, finding relevant information means finding images (that refer back to a given video) which relate to our input image by comparing these features. By utilizing video frames in addition to the thumbnails, we can determine the time within the video which is relevant, from the number of frame, as we grab one frame every second.

With a Convolutional Neural Network, we have 3 important layers which are the Convolutional Layer, Pooling Layer, and Activation Layer. While the Fully Connected Network is important in most neural networks, as we want the low-level image representation and not the high-level classification the model makes, we remove this layer. Convolution refers to the combination of two functions, and is often used in Probability Distributions; in the case of our CNN, the convolutional combines the input information (in the form of raw pixel values) with the values of a sliding filter, merging them together. The filters that we choose, or are pre-trained normally, will affect the image in different ways in order to highlight different characteristics of the image. 

Suppose we have a matrix F of raw pixels and our filter matrix G. To compute the convolution or merge the raw pixels and filter matrix G together, we compute the following grid square for our output feature map $H$ at a row $m$ and column $n$:

\[ H(m, n)  =  (F * G)(m, n) = \sum_{0}^{j} \sum_{0}^{k} G(j, k) * F() \]

Depending on parameters of stride, kernel size, and padding around the image, we get a output feature map which represents the merging of the image and filter. This becomes important, depending on the filter we choose. For example, the Sobel Filter \cite{} acts as a edge detector, where different matrices or kernels can detect different edges. So, when our image is passed through this layer, it essentially highlights the edges of the image, which I'll describe as "activations" of that feature being an edge. With these activations of these features, the CNN can learn information such as edges, textures, colors, and other characterisitics and combine them to make decisions on what features and/or objects it sees. Overtime, through passing these features further into convolutional layers, the model can take lower-level features (such as edges) and begin to combine and get activations of higher-level features (such as nose, head, eyes) until it has a very semantic representation of the image

The unsung hero of the CNN is the pooling layer. The pooling layer acts as a feature "summarizer", where it grabs the maximum value of a group (2x2, 3x3) of grid squares (or pixels) and uses this as one grid square of the output. In a similar fashion to the kernel in the convolutional layer, the pooling layer will be simply "slide" over the image and compute the maximum of the activations of features it sees. There are several things this layer does: it reduces the size of the input overtime, reducing it to something more readable (highlighting where and how much a feature is present), reduces the overall computation (since with our deeper levels, we only look at these summarized activations as opposed to the entire image), and it can tells us spatial information (which is important in the context of an image) which helps the model perform well in conditions where the image is translated (moved side to side) or rotated as the positions of features will change (but the activations will not).

Finally, we have the activation layer. Depending on the choice of activitation, the model will either highlight the maxima and/or minima. As many models have had success with the ReLU activation layer \cite{}, it is the favorite and the Xception model includes this layer as well. It will alter the values of the outputs of the convolutional and pooling layers, by either storing a value of 0 or the value (if the value is higher than 0). The activation layer essentially captures what features exists, as opposed to unneccesaary information (such as where features don't exist) \cite{}. By doing so, we summarize the image features further for the next layer.

Moving on, the Faster R-CNN is a third-generation object detection model, sprouting from the original R-CNN. The Faster R-CNN utilizes a Region Proposal Network (RPN) and convolutional layers (as discussed above) to 

\section{Prior Work}

Many image-related applications have been developed around gathering information. A popular application is Google Lens, an application which in real time can analyze an image (whether it has text or a given object) and identify the object or text. Furthermore, image-related search applications, such as Google’s reverse image search, are very useful in determining the source of an image or determining similar images based on an input image. In fact, it’s being used often for educational purposes such as with plant identification \cite{Moore2018}. In a similar way to how Google Lens and Google’s reverse image search aims to solve the information gap (given only visual context), my application serves to solve it using related videos as opposed to a purely trained AI model on the classification of objects.

To go on, researchers at Stanford have utilized video frames within a given video (along with labeled good and bad thumbnails) to train a convolutional neural network to determine what frames of that video would constitute as a good “thumbnail”, or image preview to capture a user’s attention \cite{Stanford2017}. Extending this idea, we can utilize the video frames in order to gather images to act as “markers” for the videos; when an user-given image matches, it references the video and/or the time within it.

Along with classifying YouTube thumbnails together, neural networks have also proven to be able to learn to cluster YouTube thumbnails together, categorizing them into different categories without the help of tags (i.e. labeled categories on the videos). Utilizing k-means clustering, they were able to compare and determine that a clustering algorithm could categorize videos by their thumbnails as well as YouTube did manually with tags \cite{Stanford2021}. As my dataset is not labeled and neural networks have shown to categorize based on features as well (with clustering), likewise I'll extend this idea to segment images and develop the clusters/categories in an unsupervised manner with a similar combination of neural network (CNN) and clustering method (FCM).

A final investigation of other solved problems led me to Kavitha's paper, which utilizes and recommends the usage of Fuzzy C Means for image segmentation as it has higher precision and efficiency; Kavitha utilizes FCM to gather information from image for natural disaster management \cite{Kavitha2020}. Not only do I need to group images based on image visuals, but I need an efficient method to perform it (as I only have access to Oxy's computer and my own). As a clear parallel, I can utilize Fuzzy C Means as my clustering algorithm as opposed to K-Means or other clustering methods.

% However, there are many ways (or algorithms) to cluster a dataset, depending on the type of data one is handling. Therefore, analyzing papers which tackled a similar problem led me to Kavitha’s paper, which retrieves images that are relevant to the user given image, for the purposes of getting information that’s useful for natural disaster management. The paper pushes that other image-retrieval algorithms are incomponent in terms of efficiency with clustering images, and that utilizing the Fuzzy characteristic algorithm can achieve great precision with better efficiency \cite{Kavitha2020}. Furthermore, an article about the many methods/algorithms of clustering agreed that Fuzzy algorithms are best for image segmentation which aligns with the arguments of the paper \cite{PrasadClustering}. As my project will involve deriving related images and image segmentation (in order to make the images more meaningful), my project landed on the Fuzzy C-Means Algorithm (as opposed to a K-means clustering) as the resource for clustering my images all together and creating these baskets.

\section {Methods}

Data was fetched from Google's Youtube8M Dataset, containing data in the form of tensorflow record files (*.tfrecord). These tensorflow records contained high-level features of the video (such as cateogry and a general description), and a fake id related to the YouTube video. So, using tensorflow's parser, I parsed the tensorflow record files, obtaining the fake id assigned to a video and the video's human-labeled features or overall characteristics. With the fake id, I could grab the real YouTube ID assigned to the video through a link provided by Google, and download the video by using a python module called yt-dlp. As not only grabbing the YouTube ID and corresponding YouTube video took large amounts of time, I asynchrously fetched the YouTube IDs and upon arrival (or the completion of the HTTP request), I used multithreadding to concurrently run the downloads of the YouTube videos. After obtaining the thumbnail and the corresponding video frames, I used cv2's imread and resize function to convert the thumbnail into an 3D array of width 299 pixels, height 299 pixels, and 3 channels representing the Red, Green, and Blue values to match the input expected by the Convolutional Neural Network. 

As the data consisted of images, and the image often had real-life subjects, I utilized the Convolutional Neural Network which could learn and describe characteristics of objects and regions of images. As the YouTube images are unlabeled, I wanted a CNN model that was pre-trained on images, so I didn't have to do this training myself (which would mean more data and time). A final point is that I wanted the model to be efficient, but thorough in its computation. With these benefits in mind, the Xception model was a clear choice, as I could create a pre-trained version of the model (with images from ImageNet) utilizing the keras library in Python. As well as, compared to other pre-trained (on ImageNet) models available, the Xception model was very efficient, while maintaining a very high top-five accuracy for image classification, which is important in the context of related videos (where finding many good options can be better than finding only one great option) \cite{Stancic2022}. Efficiency becomes important when computing millions of images to decrease the amount of computing power and time for the image. A Convolutional Neural Network, in its normal state, will return what objects it believes it sees (with Image Classification), but this is far too semantic and researchers found that removing the fully connected layers of the model improved the accuracy and ability to compare images based on similarity. Therefore, to construct the AlteredXception (I will call it), the Xception model was first created, and using the keras model, I removed the final layers of the Xception model, and made the final output the Pooling Layer. By having the final output being the Pooling Layer, the final output will consist of the flattened feature maps which tells us what regions have specific characteristics.

After utilizing this library, I began using the model to gather representations of various images from frames and thumbnails I gathered, passing them through the 


 
While the AlteredXception model is an excellent model in deriving features from an image, one problem is that it may see the image is one way. The max-pooling and average-pooling layers of a Convolutional Neural Network will spatially group and based on activations of neurons together, will determine features for several parts of the image. However, we could look at the image in different ways using the RMAC or Regional Maximal Activations of Convolutions. By precomputing different square regions to look over, as opposed to grabbing features and computing the vector in one way, we compute many different vectors based on different sized regions (to grab small and big features) and combine them together to form a vector which captures many different view points. Not only is this just as efficient as the normal CNN (as we just use the feature maps from the neural network), but this algorithm has proven to perform better than normal standard CNNs in image classification \cite{}.

To store the image representations the models computed in the image, I utilized my file system as a database, consisting of only JSON and NPY files. JSON files were initially used as they allowed me to maintain order and also check for duplicate predictions in my database. By storing the image paths as a key and the predictions as a value, I can simply check if an image path already exists within my JSON before computing the prediction (and wasting time creating duplicate predictions). As well as, I can use it to order my image paths and view it relatively easily. However, a big problem with JSON files is that loading the files take a long time, which in the context of an user interface and computing other predictions, can mean long wait times (on the magnitude of minutes) to load and have the prediction maps ready to be queried from or added to. So, based on research online \cite{} and the overall ease-of-use with the numpy library (as my image representations were large numpy arrays), .npy files were used to store the image representations. To maintain the benefits of JSON with duplicates, I created a .npy file for the image representations (essentially a giant array of numpy arrays), and a JSON file which mapped an image path to the index of where the corresponding image representation is within the numpy array. So, the numpy files are faster to load and store, are smaller in size (as numpy has methods to efficiently store their arrays in npy files), and we still gain the benefits of JSON! For the Object Detection Model (unlike the other models), inverted indexing could be utilized for faster querying. Inverted Indexing is the process of storing "words" to image as a way to determine and find more related images quickly. If our query image has visual words computed, we can use them to find other related images based on matching object classes. In the case of the Faster-R-CNN model, images had representations consisting of the objects' classes, sizes, and the model's confidence scores. Therefore, we can use inverted indexing to store an object class and map it to image paths which had the same object class. Each image path would also map to its confidence score of that object classes, so when we query, we can favor images where the model was more confident. By using this style of "database", when we compute the objects of an image, we can use the objects we find to query for images (and therefore videos) with related objects very quickly (since we don't look at all images, only those which match).

In combination with the XceptionModel, a pre-trained Object Detection Model, called the Faster R-CNN model, was utilized for its accuracy and ease-of-use with general object classes. While it couldn't pick up on nuances very well, it can be used as a discriminator as well as for grabbing relevant (but not specific) videos very easily. 

After attaining the descriptions of our YouTube images through these models and storing them away, the description of the query image will be computed using the same methods. To compare the descriptions created by the CNNs, I utilized Euclidean distance, or the sqaure root of the sum of the squared differences between the vectors. With this distance, we are essentially computing the difference of features within regions of the image. If there are more regions with different characteristics between images, they are viewed as farther in distance. If regions has similar characteristics in many regions, they were viewed as closer together. For descriptions created by the Object Detection Model, I computed a score based on the number of matching object classes and the difference between the number of objects (per class) in the query image and the number of objects in the database image. With similar images, they should have the same number of matching objects (images with two dogs should return images with two dogs). As well as, the number of matching classes matter (images with a dog and bike should return images with a dog and bike and not just dogs or bikes). By computing these differences, we return the relevant videos based on a high object score or a low Euclidean distance. 

With the comparison in place, we have a way to get related videos form the query video. To make and visualize this system, I utilized flask, a Python library, which can quickly make a web server to serve HTML, CSS, and JS files to show the relevant videos and provide an input to take in query images. Utilizing HTML and CSS, I created a base website which would take the query image and send a POST request to my web server with the image. With the query image from the user, I would compute the image and return the relevant videos for each model in the form of JSON. Utilizing Javascript, I created iframes and section to store each individual video and individual times (which became clickable buttons to go to that time of the video) for each video. Not only does this user interface showcase the models and database, but it provides a preview into the benefits of image search by including things like relevant times. 

\section{Evaluation Metrics}

To test the evaluation of this image search tool, I tested common conditions where upon a given query image, the difference between what we predicted and what the model predicted. In the context of any search tool, an important metric is similarity and the ability to pick up nuances and return relevant and specific images, either in the context of a small or large number of possibilities. As an example, if we submit an image of a golden retriever into a dog database, we should expect the model to return golden retriever videos, and not only just dog videos.

To test the model, we utilized data collection, data labeling, and pre-determined query images. For dogs and games, I downloaded one hundred random videos from YouTube8M's dataset of that corresponding category, downloading the frames and thumbnails as well. Afterward, I hand-labeled with descriptions of the video's thumbnail and overall video contents, to the best of my ability. Using these descriptions and after picking a query image (that had many possible relevant videos), I would determine, based on my own descriptions, what the top 5-10 most relevant videos were based on my pre-determined query image. 

After having the models compute the image representations of the frames and thumbnail, the query images were queried inside these small database, and the top 10 videos were returned from the model. To determine how well the model performed, we compared and determined how many of the top 10 videos the model predicted matched the ground truth "videos" I had picked previously. Based on the models' scores, we can deterime which model and/or algorithm (such as the normal CNN versus the RMAC algorithm), performed better.

While placing in normal images is a great metric, YouTube videos, with the goal of attracting or keeping viewers, often will not have just plain images as their videos' thumbnail nor frames, but will have text or have visuals (such as red arrows) to direct and grab the users' attention. So, to account for this image manipulation, distorted and "clickbait" query images (by adding text and red arrows to the image in likely positions) were also utilized. To distort the images, a simple online image distorter (https://www.imgonline.com.ua/eng/picture-distortion.php) was used with an image distortion intensity of 10. To add "clickbait" to the images, another online tool (https://www.visualwatermark.com/add-text-to-photos/) was used, where hyperbole text, such as "Golden Retriever Goes Animal", relating the image and red arrows were added to the image.

Once the different query images were pushed through, the results were returned and placed inside of an Excel spredsheet for ease of use and storing. The graphs were plotted to compare the results under the different conditions and to determine which models performed better.

Other conditions such as "reverse image search" and drawings were tested as well. To determine if the model can return the video based on one of its own frames, an individual frame was picked from the video and passed through to see if the corresponding video would show (both in the small and large database context). As well as, drawings of both my own and from Google Images were queried into a small database of art videos to see if the models could find the relevant videos based on non-"real" objects.

\section{Results and Discussion}

My solution, in its current state, is not viable. With small databases of one hundred videos, it performed very well, grabbing several top picks. With dog, game, and art videos, the 

As well as, when image manipulation was introduced in the form of distortion, visuals, and text, the models performed only slightly worse, losing one or two relevant picks. 


\section {Ethical Considerations}

% - less extreme on proposal of ethics
% - that are most relevant 
% - dw about image manipulation
% - (dias)selection bias
% - warning based on keywords of videos
% 	- human segmentations?

% The project is considered both in its complete technological and societal context. 
% Issues of bias and diversity are explored in detail, and potential contributions to global and local inequity examined. 
% Relevant literature is cited.

In consideration of the ethical aspects of this project, there are aspects of the data and algorithms which have bias including the Youtube 8M Dataset and clustering algorithm.

% In the process of creating this project, there are ethical concerns of this project such as the bias within the data used for the CNN and clustering, as well as, a lack of explainability and transparency entirely behind my algorithm.

To begin with, the YouTube 8M Dataset primarily contains selection bias and sample bias due to the limited amount of data and diversity surrounding video platforms and YouTube 8M’s dataset. Selection bias, to be general, is the difference in characteristics between those selected for a study (training data) and those not (test / real world data) \cite{Yu2020}; YouTube’s 8M Dataset, which contains 6.1 million videos from 2017 and 2018 \cite{googleYT8M}, has much less data compared to the estimated 800 million on Youtube. While the dataset contains the data across the same amount of general categories (15), the distribution of data across these categories and subcategories is weaker, primarily having videos in the games or vehicle category \cite{googleYT8M}. This combination of lack of data and diversity within the data contributes to poor generalization, or poor performance on the test set (real world) in comparison to the training set, due to the model overfitting to the training data and struggling to account for new characteristics from new input images \cite{Yu2020}. The main reason behind poor generalization in selection bias comes from unwanted correlations between features or characteristics in images; clustering on images with these correlated features lead to “spurious (fake) relationships” \cite{Yu2020}. Furthermore, due to the selection of data by YouTube, the YouTube dataset has sample bias and a favoring toward Western ideals. To reiterate, the videos from the dataset are primarily distributed in games, vehicles, and concerts with around 780,000 videos, whereas only 3000 videos are available about Indian cuisine and TV. The major advertising audience from YouTube is aged 25-34, male, and from India \cite{HootSuite2022}; despite this, there is a disparity in content and data surrounding India. Part of the reason behind the creation of the dataset was addressing the need for “large-scale, labeled datasets” and “[accelerating] research on large scale video understanding” \cite{Warrick2020}. As gaming and car content thrives on YouTube, this data may’ve been curated to not only appeal to Western developers, but to bring research on the largest sources of videos. Additionally, as the data is human-labeled \cite{googleYT8M}, only those from Google who had the technology and expertise to label the data did so. In the process, this disfavors content and people (who would label) from other backgrounds and lessens the diversity of the data. This selection of data affects the clustering algorithm in a similar way to sample bias, or bias due to nonrandom data. Like selection bias, sample bias will cause correlation between features; as non-popular videos (due to language or cultural barriers) are less likely picked (and have small groups), it’s less likely the clustering algorithm will create a separate cluster for them. In the process, the algorithm is less likely to group objects and people from lesser-known videos/backgrounds together, instead placing them in a larger cluster (from some other identifying feature) with less related videos.

As a final point, due to the lack of explainability behind the machine learning algorithm’s decisions, finding and fixing the sources of unethicality (bias) can become an ethical issue. While many machine learning algorithms, Fuzzy C-Means included, are mathematically explainable in how they converge or determine clusters, these explanations don’t describe why they decided on a specific set of clusters. As a consequence of this implementation, fixing a known source of bias and error, through fixes in the implementation, is difficult; telling the computer where the centers should be removes the machine learning aspect of this project. As well as, the high number of dimensions (\(>100\)) I'll be clustering over may become hard to conceptualize, understand, and explain. Therefore, unwanted biases may occur in the results of my project (and have simply make the biases previously stated worse), with no guarantee of being able to fix them before or after deployment. 

% Selection bias, to be general, is the difference in characteristics between those selected for a study (training data) and those not (test / real world data) \cite{Yu2020}; it primarily occurs due to an underrepresentation of the population from the sample. YouTube’s 8M Dataset, which contains 6.1 million videos from 2017 and 2018 \cite{googleYT8M}, has much less data compared to the estimated 800 million on Youtube. While the dataset contains the data across the same amount of general categories (15), the distribution of data across these categories and subcategories is weaker, primarily having videos in the games or vehicle category \cite{googleYT8M}. While YouTube has the same number of categories, they are meant to encompass many of the videos together; the subcategories (and more niche categories) of YouTube beneath these general categories are not shown or displayed (more inferred and created by subcommunities). This combination of lack of data and diversity within the data contributes to poor generalization, or poor performance on the test set (real world) in comparison to the training set, due to the model overfitting to the training data and struggling to account for new characteristics from new input images \cite{Yu2020}. The main reason behind poor generalization in selection bias comes from unwanted correlations between features or characteristics in images; clustering on images with these correlated features lead to “spurious (fake) relationships” \cite{Yu2020}. A great example is working with an image dataset containing images of mainly dogs laying in grass. Upon the introduction of an image of a cat laying in grass, due to the high correlation between dogs and grass (implicitly in the data), the cat gets placed in the wrong cluster with the dogs \cite{Wang2020}. In a similar fashion, any unintentional and intentional biases of the YouTube 8M dataset will be exacerbated, by the model conforming to what it believes is the population (training data) and forming relationships (such as between dogs and grass) about this data which don’t exist on all of YouTube. The subtypes of selection bias, historical and sample bias, can further describe the specific limitations and contribution effects of bias in our data and why our data cannot be ethical when used for machine learning.

% Finally, due to the selection of data by YouTube, the YouTube dataset has sample bias and a favoring toward Western ideals. To reiterate, the videos from the dataset are primarily distributed in games, vehicles, and concerts with around 780,000 videos, whereas only 3000 videos are available about Indian cuisine and TV. The major advertising audience from YouTube is aged 25-34, male, and from India \cite{HootSuite2022}; despite this, there is a disparity in content and data surrounding India. Part of the reason behind the creation of the dataset was addressing the need for “large-scale, labeled datasets” and “[accelerating] research on large scale video understanding” \cite{Warrick2020}. As gaming and car content thrives on YouTube, this data may’ve been curated to not only appeal to Western developers, but to bring research on the largest sources of videos. Additionally, as the data is human-labeled \cite{googleYT8M}, only those from Google who had the technology and expertise to label the data did so. In the process, this disfavors content and people (who would label) from other backgrounds and lessens the diversity of the data. This selection of data affects the clustering algorithm in a similar way to sample bias, or bias due to nonrandom data. Like selection bias, sample bias will cause correlation between features; as non-popular videos (due to language or cultural barriers) are less likely picked (and have small groups), it’s less likely the clustering algorithm will create a separate cluster for them. In the process, the algorithm is less likely to group objects and people from lesser-known videos/backgrounds together, instead placing them in a larger cluster (from some other identifying feature) with less related videos. Additionally, the algorithm may choose to focus on a well-known feature of a lesser-known video as opposed to a lesser-known, identifying feature of the video or thumbnail. As a result, finding related videos for an object or person not recognized in the United States becomes more difficult (and may even require parsing through the relevant videos by the user which defeats the purpose). Evidently, the unintentional biases present within my data contribute to the algorithm making incorrect relationships in the data and unethical decisions (clusters) based on its interpretation of the population.

% As a final point, due to the lack of explainability behind the machine learning algorithm’s decisions, finding and fixing the sources of unethicality (bias) become too tedious and difficult to handle. While many machine learning algorithms, Fuzzy C-Means included, are mathematically explainable in how they converge or determine clusters, these explanations don’t describe why they decided on a specific set of clusters. For the Fuzzy C-Means algorithm, as the initial centers of centroids are picked at random and the placement of these initial centers affects the end result, there’s no way to backtrack mathematically. The algorithm can perform very well or very poorly, and the best method of overcoming this simply involves recomputing the clusters until the error (or distance between the cluster’s centroid and its corresponding distance) is minimized \cite{GeeksForGeeks2019}. As a consequence of this implementation, fixing a known source of bias and error, through fixes in the implementation, is difficult; telling the computer where the centers should be removes the machine learning aspect of this project. As well as, if and when my algorithm acts unfairly, I cannot determine the source of its decision whether it’s due to the data or implementation of my algorithm. While I can reiterate and determine a relatively good set of clusters (using methods like the Elbow method to evaluate them \cite{PrasadClustering}), this doesn’t ensure optimal or unbiased results. This translates to deployment where the algorithm may perform horribly and under new data, return unexpected results without any sort of way to determine where it went wrong. The main solution to this problem is either removing the root cause (if it’s findable), or removing the project from deployment immediately (like with Microsoft’s Zo). As finding the root cause of an unethical decision/result is difficult, neither of these decisions are sufficient for completing and deploying my project. Either I have to micromanage the algorithm (which defeats the purpose of it being machine learning), or I have to start from scratch with the data and algorithm (which already has presented problems). In total, unintentional and intentional data biases present themselves as unavoidable and difficult ethical issues to fix; finding and fixing all of these is not doable given the scope of my project.

\printbibliography
\end{document}